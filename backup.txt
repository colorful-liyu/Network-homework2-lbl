parser = argparse.ArgumentParser(description='PyTorch Training')

# data generation
parser.add_argument('--mode', type=int, default=4, help='which to be target domain')
parser.add_argument('--seed', type=int, default=233, metavar='S',
                    help='random seed (default: 1)')
parser.add_argument('--num-workers', type=int, default=4, metavar='N')
parser.add_argument('--batch-size', type=int, default=128, metavar='N',
                    help='input batch size for training (default: 128)')

# network arch
parser.add_argument('--alpha', type=float, default=0.9, help='bp alpha')
parser.add_argument('--lamda', type=float, default=1., help='loss weight')
parser.add_argument('--src_only_flag', action='store_true', default=False,
                    help='baseline')

# training setting
parser.add_argument('--epochs', type=int, default=300, metavar='N',
                    help='number of epochs to train')
parser.add_argument('--lr', type=float, default=1e-5, metavar='LR',
                    help='learning rate')
parser.add_argument('--momentum', type=float, default=0.9, metavar='M',
                    help='SGD momentum')
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')

# show and save
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
parser.add_argument('--test-interval', type=int, default=5, metavar='N',
                    help='how many epoch to wait before testing')
parser.add_argument('--save-freq', '-s', default=100, type=int, metavar='N',
                    help='save frequency')
parser.add_argument('--prefix', type=str, default='mode4-baseline',
                    help='special name')


import torch
from torch import nn
from torch.autograd import Function


class GRL(Function):

    @staticmethod
    def forward(ctx, input, alpha):
        ctx.alpha = alpha
        return input.view_as(input)

    @staticmethod
    def backward(ctx, grad_output):
        output = ctx.alpha * grad_output.neg()
        return output, None


class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.encoder = nn.Sequential(
            # nn.Linear(310, 310), nn.BatchNorm1d(310), nn.ReLU(), 
            nn.Linear(310, 128), nn.BatchNorm1d(128), nn.ReLU(), 
            nn.Linear(128, 128), nn.BatchNorm1d(128), nn.ReLU(), 
        )

    def forward(self, x):
        return self.encoder(x)

class DANN(nn.Module):
    def __init__(self):
        super(DANN, self).__init__()
        self.encoder = Encoder()
        self.classifier = nn.Sequential(
            nn.Linear(128, 64), nn.ReLU(),
            # nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, 3)
        )
        self.discriminator = nn.Sequential(
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, 2)
        )

    def forward(self, x, alpha=1.):
        feature = self.encoder(x)
        feature_reverse = GRL.apply(feature, alpha)
        return self.classifier(feature), self.discriminator(feature_reverse)


class BaselineModel(nn.Module):
    def __init__(self):
        super(BaselineModel, self).__init__()
        self.encoder = Encoder()
        self.classifier = nn.Sequential(
            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), 
            # nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),
            nn.Linear(64, 3)
        )

    def forward(self, x):
        return self.classifier(self.encoder(x))

